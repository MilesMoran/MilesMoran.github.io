
<!-- saved from url=(0047)http://wight.seg.rmit.edu.au/fscholer/anova.php -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<link rel="stylesheet" type="text/css" href="./RMIT-Scholer-ANOVA-SumSquares_files/stylefile.css">
<title>ANOVA</title>
</head>

<body>
<!--
background="img/bricks.gif">
-->

<h1>ANOVA (and R)</h1>

<hr>


<h2>The ANOVA Controversy</h2>

<p>
ANOVA is a statistical process for analysing the amount of variance that
is contributed to a sample by different factors.  It was initially
derived by R. A. Fisher in 1925, for the case of balanced data (equal numbers of
observations for each level of a factor).
</p>

<p>
When data is unbalanced, there are different ways to calculate the sums
of squares for ANOVA. There are at least 3 approaches,
commonly called <i> Type I, II and III</i> sums of squares (this
notation seems to have been introduced into the
statistics world from the SAS package but is now
widespread). Which type to use has led to an ongoing controversy in the field of statistics
(for an overview, see Heer [2]).
However, it essentially comes down to testing different hypotheses about the data.  
</p>

<hr>

<h2>Type I, II and III Sums of Squares</h2>

<p>
Consider a model that includes two factors <tt>A</tt> and <tt>B</tt>; there are therefore two main effects,
and an interaction, <tt>AB</tt>. The <i>full</i> model is represented by
<tt>SS(A, B, AB)</tt>.
</p>

<p>
Other models are represented similarly: <tt>SS(A, B)</tt> indicates the model
with no interaction, <tt>SS(B, AB)</tt> indicates the model that does not account
for effects from factor A, and so on.
</p>

<p>
The influence of particular factors (including interactions) can be
tested by examining the <i>differences</i> between models.
For example, to determine the presence of an interaction effect, an <i>F-test</i> of the models
<tt>SS(A, B, AB)</tt> and the no-interaction model <tt>SS(A, B)</tt> would be carried out.
</p>

<p>
It is convenient to define <i>incremental</i> sums of squares to represent these differences. 
Following the notation of Fox [1], let 
</p><pre>SS(AB | A, B) = SS(A, B, AB) - SS(A, B)
SS(A | B, AB) = SS(A, B, AB) - SS(B, AB)
SS(B | A, AB) = SS(A, B, AB) - SS(A, AB)
SS(A | B)     = SS(A, B) - SS(B)
SS(B | A)     = SS(A, B) - SS(A)
</pre>
The notation shows the incremental differences in sums of squares,
for example <tt>SS(AB | A, B)</tt> represents "the sum of squares for
interaction <i>after</i> the main effects", and <tt>SS(A | B)</tt> is "the sum of
squares for the <tt>A</tt> main effect <i>after</i> the <tt>B</tt> main effect and ignoring
interactions" [1].
<p></p>

<p>
The different types of sums of squares then arise depending on the stage of model
reduction at which they are carried out. In particular:

</p><ul>
<li>
Type I, also called "sequential" sum of squares:
<ul>
<li><tt>SS(A)</tt> for factor A.</li> 
<li><tt>SS(B | A)</tt> for factor B.</li> 
<li><tt> SS(AB | B, A)</tt> for interaction AB.</li> 
<li>This tests the main effect of factor <tt>A</tt>, followed by the main effect of factor <tt>B</tt> 
<i>after</i> the main effect of <tt>A</tt>, followed by the interaction effect <tt>AB</tt> <i>after</i> the main effects.</li>
<li> Because of the sequential nature and the fact that the two main factors are 
tested <i>in a particular order</i>, this type of sums of squares will give different results for 
unbalanced data depending on which main effect is considered first.</li>
<li>For unbalanced data, this approach tests for a difference in the <i>weighted</i> marginal means. 
In practical terms, this means that the results are dependent on the realized sample sizes, namely 
the proportions in the particular data set. 
In other words, it is testing the first factor without <i>controlling</i> for the other factor (for 
further discussion and a worked example, see Zahn [4]). </li>
<li>Note that this is often <b>not</b> the hypothesis that 
is of interest when dealing with unbalanced data. </li> 

</ul>
</li>
<li>
Type II: 
<ul>
<li><tt>SS(A | B)</tt> for factor A.</li>
<li><tt>SS(B | A)</tt> for factor B.</li>
<li>This type tests for each main effect <i>after</i> the other main effect.</li>
<li>Note that <i>no significant interaction</i> is assumed
(in other words, you should test for interaction first (<tt>SS(AB | A, B)</tt>) and 
only if <tt>AB</tt> is not significant, continue with the analysis for main effects).</li>
<li>If there is indeed no interaction, then type II is statistically more powerful than type III (see Langsrud [3] for further details).</li>
<li>Computationally, this is equivalent to running a type I analysis with different orders of the factors, and taking the appropriate output (the second, where one main effect is run <i>after</i> the other, in the example above).</li>
</ul>
</li>
<li>
Type III: 
<ul>
<li><tt>SS(A | B, AB)</tt> for factor A.</li>
<li><tt>SS(B | A, AB)</tt> for factor B.</li>
<li>This type tests for the presence of a main effect <i>after</i> the other main effect and interaction.
This approach is therefore valid in the presence of significant interactions.</li>
<li>However, it is often not interesting to interpret a main effect if interactions are present (generally speaking, if a significant interaction is present, the main effects should not be further analysed).</li>
<li>If the interactions are not significant, type II gives a more powerful test.</li>
</ul>
</li></ul>
<p></p>

<p>
When data is balanced, the factors are <i>orthogonal</i>, and types I, II and III all give the same results.
</p>

<h3>Summary</h3>
<ul>
<li>Usually the hypothesis of interest is about the significance of one factor while <i>controlling</i> for the level of the other factors. If the data is unbalanced, this equates to using type II or III SS.</li>
<li>In general, if there is no significant interaction effect, then type II is more
powerful, and follows the principle of marginality.</li>
<li>If interaction is
present, then type II is inappropriate while type III can still be used, but
results need to be interpreted with caution (in the presence of interactions, main effects are rarely interpretable).</li>
</ul>

<hr>

<h2>The <tt>anova</tt> and <tt>aov</tt> Functions in R</h2>

<p>
The <tt>anova</tt> and <tt>aov</tt> functions in R implement a <i>sequential sum of
squares</i> (type I). As indicated above, for unbalanced data, this rarely tests a hypothesis
of interest, since essentially the effect of one factor is calculated based
on the varying levels of the other factor. In a practical sense, this
means that the results are interpretable only in relation to the
particular levels of observations that occur in the (unbalanced) data
set.  
Fortunately, based on the above discussion, it should be clear that it is relatively straightforward 
to obtain type II SS in R.
</p>

<h3>Type II SS in R</h3>
<p>
Since type II SS tests each main effect after the other main effects, and assumes no interactions, 
the correct SS can be obtained using <tt>anova()</tt> and varying the order of the factors.
</p>
<p>
For example, consider a data frame (<tt>search</tt>) for which the response variable is the
time that it takes users to find a relevant answer with an information
retreival system (<tt>time</tt>). The user is assigned to one of two experimental 
search systems on which they run the test (<tt>sys</tt>). 
They are also assigned a number of different search queries (<tt>topic</tt>).
</p>
<p>
To obtain type I SS:
</p>
<p>
</p><pre>anova(lm(time ~ sys * topic, data=search))
</pre>
<p></p>

<p>
If the data is <i>unbalanced</i>, you will obtain slightly different results if you instead use:
</p>
<p>
</p><pre>anova(lm(time ~  topic * sys, data=search))
</pre>
<p></p>

<p>
The type II SS is obtained by using the second line of output from each
of the above commands (since in type I SS, the second component will be
the second factor, <i>after</i> the first factor).
That is, you obtain the type II SS results for <tt>topic</tt> from the first command,
and the results for <tt>sys</tt> from the second.
</p>


<h3>Type III SS in R</h3>

<p>
This is slightly more involved than the type II results. 
</p>
First, it is necessary to set the <tt>contrasts</tt> option in R.
Because the multi-way ANOVA model is over-parameterised, it is necessary to choose a contrasts
setting that sums to zero, otherwise the ANOVA analysis will give incorrect results with respect 
to the expected hypothesis.
(The default contrasts type does <b>not</b> satisfy this requirement.)
<p></p>
<p>
</p><pre>options(contrasts = c("contr.sum","contr.poly"))
</pre>
<p></p>
<p>
Next, store the model:
</p>
<p>
</p><pre>model &lt;- lm(time ~  topic * sys, data=search)
</pre>
<p></p>
<p>
Finally, call the <tt>drop1</tt> function on each model component:
</p>
<p>
</p><pre>drop1(model, .~., test="F")
</pre>
<p></p>
<p>
The results give the type III SS, including the p-values from an F-test.
</p>


<hr>

<h2>Type II and III SS Using the <tt>car</tt> Package</h2>
<p>
A somewhat easier way to obtain type II and III SS is through the 
<a href="http://cran.r-project.org/web/packages/car/index.html">car package</a>. 
This defines a new function, <tt>Anova()</tt> (note the capital "A"), which can calculate type II and III SS directly.
</p>
<p>
Type II, using the same data set defined above:
</p>
<p>
</p><pre>Anova(lm(time ~  topic * sys, data=search), type=2)
</pre>
<p></p>

<p>
Type III:
</p>
<p>
</p><pre>Anova(lm(time ~  topic * sys, data=search, contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)
</pre>
<p></p>

<p>
Due to the way in which the SS are calculated when incorporating the interaction effect, 
for type III you <b>must</b> specify the contrasts option to obtain sensible results (an explanation 
is given <a href="http://www.mail-archive.com/r-help@stat.math.ethz.ch/msg69781.html">here</a>).
</p>


<hr>

<h2>References</h2>
<p>
[1] John Fox. "Applied Regression Analysis and Generlized Linear Models", 2nd ed., Sage, 2008. 
</p>
<p>
[2] David G. Herr. "On the History of ANOVA in Unbalanced, Factorial Designs: The First 30 Years",
The American Statistician, Vol. 40, No. 4, pp. 265-270, 1986.
</p>
<p>
[3] Oyvind Langsrud.
"ANOVA for unbalanced data: Use Type II instead of Type III sums of squares",
Statistics and Computing, Volume 13, Number 2, pp. 163-167, 2003.
</p>
<p>
[4] Ista Zahn. "Working with unbalanced cell sizes in multiple regression with categorical predictors", 2009.
prometheus.scp.rochester.edu/zlab/sites/default/files/InteractionsAndTypesOfSS.pdf
</p>




</body></html>