[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "As a graduate teaching assistant at OSU, I’ve taught labs and developed course materials for several of the applied classes our department offers. So far, these include\n\nST 351 - INTRODUCTION TO STATISTICAL METHODS I\nST 352 - INTRODUCTION TO STATISTICAL METHODS II\nST 511 - METHODS OF DATA ANALYSIS I\nST 512 - METHODS OF DATA ANALYSIS II\nST 509 - CONSULTING PRACTICUM\n\nSince Fall of 2023, I’ve been applying my knowledge as a Statistical Consultant for our department’s internal consulting service."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Generating Random Fields\n\n\n\nR\n\n\nSimulation\n\n\nComputation\n\n\nData Visualization\n\n\n\nHow do researchers generate high-dimensional normal variates for use in simulation studies?\n\n\n\n2022-12-07\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/projects/multivar-proj/index.html#motivation",
    "href": "posts/projects/multivar-proj/index.html#motivation",
    "title": "Contextualizing Multivariate Analysis With Wine",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "posts/projects/multivar-proj/index.html#methods",
    "href": "posts/projects/multivar-proj/index.html#methods",
    "title": "Contextualizing Multivariate Analysis With Wine",
    "section": "Methods",
    "text": "Methods"
  },
  {
    "objectID": "posts/projects/multivar-proj/index.html#simulation",
    "href": "posts/projects/multivar-proj/index.html#simulation",
    "title": "Contextualizing Multivariate Analysis With Wine",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miles Moran",
    "section": "",
    "text": "Miles Moran\nPhD Student / Consultant / TA\nOregon State University\n\n\n\n\n\nBiography\nI’m a statistician by trade, currently pursuing a PhD and Oregon State University. Under the supervision of Rob Trangucci and Lisa Madsen, our research involves the use of selection-model methodology to make spatio-temporal epidemiology models robust to MNAR data.\nI work as a statistical consultant for OSU grad students, which has really fueled my passion for effective communication/education in statistics. Consulting is one of my responsibilities as the teaching assistant for our department’s consulting practicum course; but, I also get to advise other statisticians in their own consulting projects and develop course materials for them to use.\nAlthough I like to clear my head by hiking or rock climbing, I spend a lot of my free time playing strategy games and building with LEGO.\n\n\n\n\n\n\n\n\nResearch Interests\n\nSpatial/Spatio-Temporal Models\nExtreme-Value Models\nStatistics in Climate Science\nStatistics in Epidemiology\nInteractive Data Visualization\n\n\n\n\n\n\nEducation\n\nPhD, Statistics (In Progress)\nOregon State University\nMS, Statistics (2023)\nOregon State University\nBS, Applied Mathematics (2020)\nUniversity of Nevada, Reno"
  },
  {
    "objectID": "posts/projects/pcs-proj/index.html#motivation",
    "href": "posts/projects/pcs-proj/index.html#motivation",
    "title": "Generating Random Fields",
    "section": "Motivation",
    "text": "Motivation\nThe Multivariate Normal (MVN) distribution is ubiquitous in Statistics, partly because of its “nice” properties and partly because of the asymptotic results surrounding it (e.g. the CLT). It comes as no surprise, then, that generating pseudorandom vectors from a MVN distribution is a common task in both research and industry. If we wish to apply a statistical method to a large/high-dimensional dataset, it’s important that we assess our method using an equally-large simulated dataset with similar (but known) properties.\nIn particular, many disciplines use models involving Gaussian Random Fields (GRFs). A GRF can be thought of as a MVN vector where each component has some known topological information associated with it – e.g. its location in Euclidean space. We often see GRFs written as \\(\\bm{Y} = (Y_1(\\bm{x}_1), ..., Y_{p}(\\bm{x}_p))'\\), where \\(Y_{i}\\) is the (Normally-distributed) random value associated with the known location \\(\\bm{x}_i\\). This topological information is then used to define the covariance between the \\(Y_{i}\\)’s as a function of the distance between their \\(\\bm{x}_i\\)’s. For example:\n\n\n\n\n\n\nExample\n\n\n\nResearchers want to model the rainfall in Corvallis using a GRF, making the assumption that the correlation between rainfall measurements in two locations decays exponentially with their Euclidean distance. They collect data from 64 locations situated in an \\(8×8\\) grid pattern in the surrounding area:\n\nAfter which, they model the rainfall as \\(\\bm{Y} = (Y_1(\\bm{x}_1), ..., Y_{64}(\\bm{x}_{64}))' \\sim \\MVN{\\bm{μ}, \\bm{Σ}}\\) where \\[\\begin{equation}\n    \\bm{Σ}_{ij} = σ^2 \\exp{-\\frac{||\\bm{x}_i - \\bm{x}_j||}{γ}}\n\\end{equation}\\] And then, this complicated correlation structure can be determined purely by the known locations and only 2 unknown parameters, \\(σ^2\\) and \\(γ\\).\n\n\nThese GRFs are used to model, among other things,\n\nThe Cosmic Microwave Background Radiation (CMBR), which enables Cosmologists to make inference about how the universe was formed\nNeural Activity in the brain, which enables Neuroscientists to make inference about how different regions of the brain are connected\nPermeability Coefficients of Geological Formations, which enables Geologists to make inference about how matter is distributed in Earth’s crust"
  },
  {
    "objectID": "posts/projects/pcs-proj/index.html#methods",
    "href": "posts/projects/pcs-proj/index.html#methods",
    "title": "Generating Random Fields",
    "section": "Methods",
    "text": "Methods\nOn the surface, generating a realization of a GRF (or any MVN vector) seems easy. We know from the properties of Normal distributions that\n\nif \\(Z_1, ..., Z_p \\iid \\Normal{0,1}\\), then \\(\\bm{Z} = (Z_1, ..., Z_p)' \\sim \\MVN{\\bm{0}, \\bm{I}_p}\\)\nif \\(\\bm{Z} \\sim \\MVN{\\bm{0}, \\bm{I}_p}\\), then (\\(\\bm{AZ}+\\bm{b}) \\sim \\MVN{\\bm{b}, \\bm{AA}'}\\)\n\nSo, a natural first-attempt at realizing a vector \\(\\bm{Y} \\sim \\MVN{\\bm{μ}, \\bm{Σ}}\\) is as follows:\n\nGenerate \\(Z_1, ..., Z_p \\iid \\Normal{0,1}\\)\nSet \\(\\bm{Z} = (Z_1, ..., Z_p)'\\)\nCalculate the matrix \\(\\bm{A}\\) such that \\(\\bm{AA}' = \\bm{Σ}\\) \\ (the “square root” of \\(\\bm{Σ}\\), often denoted \\(\\bm{Σ}^{1/2}\\))\nSet \\(\\bm{Y} = \\bm{AZ} + \\bm{μ}\\)\n\nAlthough this method is mathematically sound, step (3) is a serious computational burden – particularly due to the dimensionality of \\(\\bm{Y}\\). In the rainfall example above, suppose instead that the researcher measured rainfall on a larger grid – say \\(100×100\\) instead of \\(8×8\\). Then, we have \\(\\text{dim}(\\bm{Y})=10,000\\) which means we have to decompose a \\(10,000×10,000\\) covariance matrix!\nSome methods for approximating \\(\\bm{Σ}^{1/2}\\) involve generalizations of univariate optimization techniques (basically minimizing the function \\(f(\\bm{A}) = \\bm{AA}' - \\bm{Σ}\\)). Two worth mentioning (despite being outdated) are the Denman-Beavers and Newton-Schultz algorithms:\nDenman-Beavers:\n\nPseudocodeRPython\n\n\n\nInitialize \\(\\bm{A}^{(0)} = \\bm{Σ}\\)\nInitialize \\(\\bm{B}^{(0)} = \\bm{I}_{p}\\)\nUntil convergence, iterate: \\[\n\\left\\{\n\\begin{aligned}\n\\bm{A}^{(k+1)} &=\n    \\left(\\tfrac{1}{2}\\right)\n    \\left( \\bm{A}^{(k)} + \\left[ \\bm{B}^{(k)} \\right]^{-1} \\right) \\\\\n\\bm{B}^{(k+1)} &=\n    \\left(\\tfrac{1}{2}\\right)\n    \\left( \\bm{B}^{(k)} + \\left[ \\bm{A}^{(k)} \\right]^{-1} \\right)\n\\end{aligned}\n\\right\\}\n\\]\nAssuming \\(K\\)-many iterations until convergence, return \\(\\hat{\\bm{Σ}}^{1/2} = \\bm{A}^{(K)}\\)  and  \\(\\hat{\\bm{Σ}}^{-1/2} = \\bm{B}^{(K)}\\)\n\n\n\n\nmatrix.sqrt.DB &lt;- function(Σ, tol) \n{\n    A &lt;- Σ\n    B &lt;- diag(nrow(Σ))\n    ε &lt;- 1\n    while(ε &gt; tol) {\n        A.old &lt;- A\n        A &lt;- (0.5)*(A.old+solve(B))\n        B &lt;- (0.5)*(B+solve(A.old))\n        ε &lt;- max(abs(A-A.old))\n    }\n    return(list(\"Σ.sqrt\"=A, \"Σ.sqrt.inv\"=B))\n}\n\n\n\n\nimport numpy as np\n\ndef matrix_sqrt_DB(Σ, tol):\n    A = Σ.copy()\n    B = np.eye(Σ.shape[0])\n    ε = 1\n    while ε &gt; tol:\n        A_old = A.copy()\n        A = 0.5 * (A_old + np.linalg.inv(B))\n        B = 0.5 * (B + np.linalg.inv(A_old))\n        ε = np.max(np.abs(A - A_old))\n    return {\"Σ.sqrt\": A, \"Σ.sqrt.inv\": B}\n\n\n\n\nNewton-Schultz:\n\nPseudocodeRPython\n\n\n\nInitialize \\(\\bm{A}^{(0)} = \\bm{Σ}/\\|\\bm{Σ}\\|_{F}\\)\nInitialize \\(\\bm{B}^{(0)} = \\bm{I}_{p}\\)\nUntil convergence, iterate: \\[\n\\left\\{\n\\begin{aligned}\n\\bm{A}^{(k+1)} &=   \n    \\left(\\tfrac{1}{2}\\right)\n    \\bm{A}^{(k)} \\left( 3\\bm{I}_{p} - \\bm{B}^{(k)} \\bm{A}^{(k)} \\right) \\\\\n\\bm{B}^{(k+1)} &=   \n    \\left(\\tfrac{1}{2}\\right)\n    \\left( 3\\bm{I}_{p} - \\bm{B}^{(k)} \\bm{A}^{(k)} \\right) \\bm{B}^{(k)}\n\\end{aligned}\n\\right\\}\n\\]\nAssuming \\(K\\)-many iterations until convergence, return \\(\\hat{\\bm{Σ}}^{1/2} = \\bm{A}^{(K)}\\sqrt{\\|\\bm{Σ}\\|_{F}}\\)  and  \\(\\hat{\\bm{Σ}}^{-1/2} = \\bm{B}^{(K)}/\\sqrt{\\|\\bm{Σ}\\|_{F}}\\)\n\n\n\n\nmatrix.sqrt.NS &lt;- function(Σ, tol) {\n    Σ.Fnorm &lt;- norm(Σ, type=\"F\")\n    A &lt;- Σ / Σ.Fnorm\n    B &lt;- I &lt;- diag(nrow(Σ))\n    ε &lt;- 1\n    while(ε &gt; tol) {\n        A.old &lt;- A\n        X &lt;- (3*I - B %*% A.old)\n        A &lt;- (0.5)*(A.old %*% X)\n        B &lt;- (0.5)*(X %*% B)\n        ε &lt;- max(abs(A.old-A))\n    }\n    return(list(\"Σ.sqrt\"=A*sqrt(Σ.Fnorm), \"Σ.sqrt.inv\"=B/sqrt(Σ.Fnorm)))\n}\n\n\n\n\nimport numpy as np\n\ndef matrix_sqrt_NS(Σ, tol):\n    Σ_Fnorm = np.linalg.norm(Σ, 'fro')\n    A = Σ / Σ_Fnorm\n    B = I = np.eye(Σ.shape[0])\n    ε = 1\n    while ε &gt; tol:\n        A_old = A.copy()\n        X = (3 * I - B @ A_old)\n        A = 0.5 * (A_old @ X)\n        B = 0.5 * (X @ B)\n        ε = np.max(np.abs(A_old - A))\n    return {\"Σ.sqrt\": A*np.sqrt(Σ_Fnorm), \"Σ.sqrt.inv\": B/np.sqrt(Σ_Fnorm)}\n\n\n\n\nFor both of these algorithms, \\((\\bm{A}^{(k)}, \\bm{B}^{(k)}) → (\\bm{Σ}^{1/2}, \\bm{Σ}^{-1/2})\\) quadratically… However, neither of these algorithms are particularly clever. They are general-purpose tools for calculating matrix square-roots, and do not leverage any of the known properties of covariance matrices. We can speed things up considerably if we notice that \\(\\bm{Σ}\\) is both symmetric and positive-semi-definite!\nIn particular, the Cholesky Decomposition and the Eigen-Decomposition are two well-studied matrix factorizations that can be applied in this scenario. These methods both involve Gaussian-Elimination-style algorithms, so their details will be omitted; but, the important part to understand is that\n\nthe Cholesky decomposition calculates \\(\\bm{Σ}^{1/2}\\) directly\nthe Eigen-decomposition calculates the matrices \\(\\bm{Q}\\) and \\(\\bm{Λ}\\) such that \\(\\bm{Σ} = \\bm{QΛQ}'\\) which can then be used to calcuate \\(\\bm{Σ}^{1/2} = \\bm{QΛ}^{1/2}\\)\n\nBecause these matrix-factorization methods are so well-studied, we know for a fact that both of them operate in \\(\\mathcal{O}(p^3)\\) time (where \\(p = \\text{dim}(\\bm{Y})\\)), meaning they scale poorly.\nIf our goal is to generate a GRF specifically, and not just any MVN vector, there is another method that does not involve finding \\(\\bm{Σ}^{1/2}\\)… Although the details are beyond the scope of this report, the idea is that \\(\\bm{Σ}\\) has additional structure to it that can be taken advantage of when the locations \\(\\bm{x}_1, ..., \\bm{x}_p\\) form a grid in Euclidean space (the most common setting for GRFs). In this case, a clever algorithm known as Circulant Embedding can be used, which relies on the Discrete Fourier Transform."
  },
  {
    "objectID": "posts/projects/pcs-proj/index.html#simulation",
    "href": "posts/projects/pcs-proj/index.html#simulation",
    "title": "Generating Random Fields",
    "section": "Simulation",
    "text": "Simulation\nTo demonstrate the difference in computational efficiency of these different MVN-generating methods, we have simulated the generation of \\(\\bm{Y} \\sim \\MVN{\\bm{μ}, \\bm{Σ}}\\) for various dimensionalities – namely, \\(p ∈ \\{10^2, 20^2, 30^2, 40^2, 50^2\\}\\) so that these methods can be compared to the GRF-specific Circulant Embedding algorithm.\nWe also compare the performance of these methods (when implemented by hand) to routines from two pre-existing R packages commonly used in Spatial Statistics : geoR::grf(), which uses the Cholesky-Decomposition method; and fields::sim.rf(), which uses the Circulant Embedding method.\nThe table below displays the computational time needed to generate 1 realization of \\(\\bm{Y} \\sim \\MVN{\\bm{μ}, \\bm{Σ}}\\) (in seconds) for each method\\(×\\)dimensionality combination, averaged across 3 simulations.\n\n\n\nSimulation Runtimes (seconds)\n \n  \n    method \n    10×10 \n    20×20 \n    30×30 \n    40×40 \n    50×50 \n  \n \n\n  \n    denman-beavers \n    0.07 \n    2.43 \n    23.74 \n    144.64 \n    - \n  \n  \n    newton-schulz \n    0.09 \n    3.02 \n    28.44 \n    148.37 \n    - \n  \n  \n    cholesky \n    0.01 \n    0.08 \n    0.41 \n    2.09 \n    6.50 \n  \n  \n    eigen \n    0.01 \n    0.25 \n    1.91 \n    9.37 \n    37.16 \n  \n  \n    geor::grf \n    0.01 \n    0.09 \n    0.53 \n    2.08 \n    6.46 \n  \n  \n    fields::sim.rf \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n    0.03 \n  \n\n\n\n\n\nNote that, for \\(p=50×50=2500\\) dimensions, the iterative methods are omitted as they would crash my R session before converging – however, we can estimate the time taken to be around 600s (10 minutes). Although not discussed previously, a rigorous comparison of these methods would also take into account the storage complexity associated with each method, since a single Covariance matrix could occupy several GB of RAM.\nPlotting these results…\n\n\n\n\n\n\n\n\n\nBefore discussing the results, note that the hand-implemented Cholesky Decomposition method is not seen on the plot above because the performance is almost identical to the geoR::grf() routine (unsurprisingly)."
  },
  {
    "objectID": "posts/projects/pcs-proj/index.html#discussion",
    "href": "posts/projects/pcs-proj/index.html#discussion",
    "title": "Generating Random Fields",
    "section": "Discussion",
    "text": "Discussion\nFrom our results above, we notice that\n\nThe Denman-Beavers and Newton-Schulz algorithms had the worst performance. This is partly because they are general-purpose tools and not problem-specific tools; but, it is also partly because the other methods are implemented more efficiently (e.g. the Cholesky and Eigen-Decomposition methods rely on well-optimized FORTRAN subroutines).\nThe Eigen-Decomposition method is noticeably slower than the Cholesky-Decomposition method (the reason why is unclear, but could likely be revealed by digging into the FORTRAN source code of the chol() and eigen() functions)\nThe Circulant-Embedding method had the best performance by far, and is likely the only method scalable for GRFs with dimensions \\(p &gt; 10,000\\). However, when running the simulations, this method was also the least stable when the \\(γ\\) parameter is large (i.e. when correlation decays very slowly with distance). From the fields::sim.rf() documentation: “The algorithm… may not always work if the correlation range is large. Specifically [when] the weight function obtained from the FFT of the covariance field [has] negative values”\n\nAlthough the simulations ran were imperfect comparisons, the conceptual takeaway is very clear: computational methods tailored to individual problems – i.e. those that take the most advantage of the problem assumptions – will tend to outperform general-purpose methods."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The following is a curated list of resources I often recommend to our consulting clients. Choose from the categories below to narrow your search:\n\nCategories:\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            An Intro to LMMs in R and SAS\n        \n\n         \n\n        \n        \n            \n            \n                Heteroskedasticity\n            \n            \n            \n                Autocorrelation\n            \n            \n            \n                LMMs\n            \n            \n            \n                R\n            \n            \n            \n                SAS\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            An Intro to R Graphics, Part I: Base Graphics\n        \n\n         \n\n        \n        \n            \n            \n                Data Viz\n            \n            \n            \n                R\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            An Intro to R Graphics, Part II: ggplot2\n        \n\n         \n\n        \n        \n            \n            \n                Data Viz\n            \n            \n            \n                R\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            Working with Factors in R: Model Parameterizations, Contrasts, Inferences on Means, etc.\n        \n\n         \n\n        \n        \n            \n            \n                DoE\n            \n            \n            \n                Contrasts\n            \n            \n            \n                R\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            Multiple Comparisons & Simultaneous Inference\n        \n\n         \n\n        \n        \n            \n            \n                DoE\n            \n            \n            \n                Contrasts\n            \n            \n            \n                Multiple Comparisons\n            \n            \n            \n                R\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Dan Hall, \n        \n        \n            Power Analysis and Sample Size Determination\n        \n\n         \n\n        \n        \n            \n            \n                DoE\n            \n            \n            \n                Contrasts\n            \n            \n            \n                Multiple Comparisons\n            \n            \n            \n                Power Analysis\n            \n            \n            \n                R\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Kristin Sainani, \n        \n        \n            The Importance of Accounting for Correlated Observations\n        \n\n         \n\n        \n        \n            \n            \n                Autocorrelation\n            \n            \n            \n                Quick-Read\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Christopher Sims, \n        \n        \n            Clustering, Random Effects, Mixed Models, Sandwiches\n        \n\n         \n\n        \n        \n            \n            \n                Heteroskedasticity\n            \n            \n            \n                Autocorrelation\n            \n            \n            \n                LMMs\n            \n            \n        \n    \n\n\n\n\n\n     \n\n    \n        \n         \n            Peter Craigmile, \n        \n        \n            A Review of Traditional Stationary Geostatistical Models\n        \n\n         \n\n        \n        \n            \n            \n                Spatial\n            \n            \n            \n                Autocorrelation\n            \n            \n        \n    \n\n\n\n\n\n\n\n\n\n\n\n  \n\nNo matching items"
  }
]